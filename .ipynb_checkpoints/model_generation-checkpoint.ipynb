{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import keras\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2027fbf265b80ac",
   "metadata": {},
   "source": [
    "data_1 = pd.read_csv('mit-bih-data-clear-data.csv')\n",
    "data_2 = pd.read_csv('ptb-diagnostic-clear-data.csv')\n",
    "data_3 = pd.read_csv('autonomic-aging-a-dataset-clear-data.csv')\n",
    "\n",
    "\n",
    "# Feature cleanup\n",
    "df = pd.concat([data_1, data_2, data_3], ignore_index=True)\n",
    "df.to_csv('heart_rate_final_data.csv', index=False)\n",
    "columns_to_drop = [\n",
    "    'patient',\n",
    "    'Max_hr_slope',\n",
    "    'mean_deviation',\n",
    "    'outliers_percentage'\n",
    "]\n",
    "\n",
    "columns_to_reduce = [\n",
    "    'vlf_power',\n",
    "    'lf_power',\n",
    "    'hf_power',\n",
    "    'highest_heart_rate',\n",
    "    'lowest_heart_rate',\n",
    "    'mean_heart_rate',\n",
    "    'median_heart_rate'\n",
    "    \n",
    "]\n",
    "\n",
    "df[columns_to_reduce] = df[columns_to_reduce].apply(lambda x: x/100)\n",
    "df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df.value_counts('diagnosis')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's check for missing values\n",
    "\n",
    "df.isnull().sum()\n"
   ],
   "id": "36942b258a8247b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace missing and infinite values with median\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)"
   ],
   "id": "d3eda68a2356aa95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Resample the data\n",
    "\n",
    "# Scenario 1: This works fine, however, this duplicates data and can lead to overfitting\n",
    "\n",
    "# from sklearn.utils import resample\n",
    "# \n",
    "# df_majority = df[df.diagnosis==1]\n",
    "# df_minority = df[df.diagnosis==0]\n",
    "# \n",
    "# df_minority_upsampled = resample(df_minority, replace=True, n_samples=487)\n",
    "# df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "# df_upsampled.value_counts('diagnosis')\n",
    "\n",
    "# Scenario 2: Let's use smote to generate synthetic data\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X = df.drop(columns=['diagnosis'])\n",
    "y = df['diagnosis']\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "df_upsampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "\n",
    "\n"
   ],
   "id": "de44a323895139d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_upsampled.value_counts('diagnosis')",
   "id": "6978c5fb86b0e323",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = df_upsampled.drop(columns=['diagnosis'])\n",
    "y = df_upsampled['diagnosis']\n",
    "\n",
    "X.head()"
   ],
   "id": "cc7484cb7eef4694",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e755ce3",
   "metadata": {},
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9eab10ee",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9d4b810",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5862c212",
   "metadata": {},
   "source": [
    "from keras import Sequential\n",
    "from keras.api.layers import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f121fb7",
   "metadata": {},
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.000001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from keras.api.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('model.keras', save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_resampled, y_train_resampled, validation_data=(X_test, y_test), epochs=3000, callbacks=[checkpoint])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Test Loss: {loss:.4f}')"
   ],
   "id": "908380c52b8a0971",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", cr)"
   ],
   "id": "1d23da69e1b60692",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visualize loss and accuracy in plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ],
   "id": "c76a293e84ed75c1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
